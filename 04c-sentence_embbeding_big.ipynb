{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://huggingface.co/LazarusNLP/all-indo-e5-small-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Revalda Putawara\\.conda\\envs\\mbgsentiment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMSTATE=241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first execution : 6 minutes\n",
    "#model = SentenceTransformer('LazarusNLP/all-indo-e5-small-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"dataset\\processed\\processed_data_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "sentences = [df.at[32,\"text\"],df.at[64,\"text\"]]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cs_embeddings'] = df['text_cleaned_stemmed'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle(r\"dataset/processed/text_embeddings.pkl\")\n",
    "#df = pd.read_pickle(r\"dataset/processed/text_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = embeddings[0] - embeddings[1]\n",
    "distance = np.sqrt(np.sum(np.square(temp)))\n",
    "\n",
    "print(\"Euclidean Distance: \", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.iloc[:,-1].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[[\"id\"]]\n",
    "df2 = pd.DataFrame(df.iloc[:,-1].to_list())\n",
    "df_emb = pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://medium.com/analytics-vidhya/topic-modelling-using-word-embeddings-and-latent-dirichlet-allocation-3494778307bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"id.stopwords.02.01.2016.txt\", \"r\")\n",
    "stopword_list = []\n",
    "for line in f:\n",
    "    stripped_line = line.strip()\n",
    "    line_list = stripped_line.split()\n",
    "    stopword_list.append(line_list[0])\n",
    "f.close()\n",
    "\n",
    "stopword_list.append(\"nya\")\n",
    "stopword_list.append(\"ya\")\n",
    "\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword_list = \"ada adalah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_stemmed</th>\n",
       "      <th>mdhugol_sentiment</th>\n",
       "      <th>mdhugol_sentiment_score</th>\n",
       "      <th>indobert_sentiment</th>\n",
       "      <th>indobert_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1880655687866552831</td>\n",
       "      <td>1880655687866552831</td>\n",
       "      <td>2025-01-18T16:37:11.000Z</td>\n",
       "      <td>Ketua Umum PDIP Megawati Soekarnoputri sempat ...</td>\n",
       "      <td>917707622026928129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ketua umum pdip megawati soekarnoputri sempat ...</td>\n",
       "      <td>ketua umum pdip megawati soekarnoputri sempat ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.989232</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1880655628819165446</td>\n",
       "      <td>1880655628819165446</td>\n",
       "      <td>2025-01-18T16:36:57.000Z</td>\n",
       "      <td>@HabilIsl Hold $WOWO dapat airdrop makan siang...</td>\n",
       "      <td>112500656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hold wowo dapat airdrop makansianggratis</td>\n",
       "      <td>hold wowo dapat airdrop makansianggratis</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.912627</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1880655540717842742</td>\n",
       "      <td>1880655540717842742</td>\n",
       "      <td>2025-01-18T16:36:36.000Z</td>\n",
       "      <td>KALAU MBG NYA INI GUE JUGA MAU DUT BIAR GUE MU...</td>\n",
       "      <td>1266052828742602752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kalau mbg nya ini gue juga mau dut biar gue mu...</td>\n",
       "      <td>kalau mbg nya ini gue juga mau dut biar gue mu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.982285</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.999828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1880655480122651014</td>\n",
       "      <td>1880655480122651014</td>\n",
       "      <td>2025-01-18T16:36:22.000Z</td>\n",
       "      <td>@neohistoria_id program MBG tuh B nya Bersyuku...</td>\n",
       "      <td>1838116873331945472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>program mbg tuh b nya bersyukur bukan bergizi</td>\n",
       "      <td>program mbg tuh b nya syukur bukan gizi</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.985092</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.996238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1880655463207117137</td>\n",
       "      <td>1880655463207117137</td>\n",
       "      <td>2025-01-18T16:36:17.000Z</td>\n",
       "      <td>@03__nakula Ga semua anak Indonesia bisa makan...</td>\n",
       "      <td>1511016593353936896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>enggak semua anak indonesia bisa makan siang ...</td>\n",
       "      <td>enggak semua anak indonesia bisa makan siang i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.564391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1                   id edit_history_tweet_ids  \\\n",
       "0             0             0  1880655687866552831    1880655687866552831   \n",
       "1             1             1  1880655628819165446    1880655628819165446   \n",
       "2             2             2  1880655540717842742    1880655540717842742   \n",
       "3             3             3  1880655480122651014    1880655480122651014   \n",
       "4             4             4  1880655463207117137    1880655463207117137   \n",
       "\n",
       "                 created_at  \\\n",
       "0  2025-01-18T16:37:11.000Z   \n",
       "1  2025-01-18T16:36:57.000Z   \n",
       "2  2025-01-18T16:36:36.000Z   \n",
       "3  2025-01-18T16:36:22.000Z   \n",
       "4  2025-01-18T16:36:17.000Z   \n",
       "\n",
       "                                                text            author_id  \\\n",
       "0  Ketua Umum PDIP Megawati Soekarnoputri sempat ...   917707622026928129   \n",
       "1  @HabilIsl Hold $WOWO dapat airdrop makan siang...            112500656   \n",
       "2  KALAU MBG NYA INI GUE JUGA MAU DUT BIAR GUE MU...  1266052828742602752   \n",
       "3  @neohistoria_id program MBG tuh B nya Bersyuku...  1838116873331945472   \n",
       "4  @03__nakula Ga semua anak Indonesia bisa makan...  1511016593353936896   \n",
       "\n",
       "   Unnamed: 0                                       text_cleaned  \\\n",
       "0         NaN  ketua umum pdip megawati soekarnoputri sempat ...   \n",
       "1         NaN           hold wowo dapat airdrop makansianggratis   \n",
       "2         NaN  kalau mbg nya ini gue juga mau dut biar gue mu...   \n",
       "3         NaN     program mbg tuh b nya bersyukur bukan bergizi    \n",
       "4         NaN   enggak semua anak indonesia bisa makan siang ...   \n",
       "\n",
       "                                text_cleaned_stemmed mdhugol_sentiment  \\\n",
       "0  ketua umum pdip megawati soekarnoputri sempat ...           neutral   \n",
       "1           hold wowo dapat airdrop makansianggratis          positive   \n",
       "2  kalau mbg nya ini gue juga mau dut biar gue mu...          negative   \n",
       "3            program mbg tuh b nya syukur bukan gizi          negative   \n",
       "4  enggak semua anak indonesia bisa makan siang i...          negative   \n",
       "\n",
       "   mdhugol_sentiment_score indobert_sentiment  indobert_sentiment_score  \n",
       "0                 0.989232            Neutral                  0.999553  \n",
       "1                 0.912627            Neutral                  0.999512  \n",
       "2                 0.982285           Negative                  0.999828  \n",
       "3                 0.985092           Negative                  0.996238  \n",
       "4                 0.990175            Neutral                  0.564391  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revalda Putawara\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words=list(my_stop_words), max_features=50000)\n",
    "tweet_matrix = tf_vectorizer.fit_transform(df['text_cleaned_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, learning_method='online', \n",
    "                                          random_state=RANDOMSTATE, verbose=0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57328553, 0.03869478, 0.30937475, 0.03904352, 0.03960142],\n",
       "       [0.50479509, 0.06925651, 0.28641409, 0.07034177, 0.06919254],\n",
       "       [0.05223168, 0.61012085, 0.23243763, 0.05223568, 0.05297416],\n",
       "       ...,\n",
       "       [0.0611912 , 0.06039162, 0.06142465, 0.06039388, 0.75659865],\n",
       "       [0.05195441, 0.05185337, 0.05215521, 0.0518546 , 0.79218243],\n",
       "       [0.22586242, 0.05969914, 0.06023805, 0.05970418, 0.5944962 ]],\n",
       "      shape=(4717, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = lda.fit(tweet_matrix)\n",
    "lda_matrix = lda_model.transform(tweet_matrix)\n",
    "lda_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['003', '01', '01rw', ..., 'zouis', 'zulhas', 'zzz'],\n",
       "      shape=(7450,), dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "triliun anggar 171 program makanbergizigratis mbg nyata keren bawa positif prabowo rupiah stunting untung banget\n",
      "\n",
      "Topic #1:\n",
      "konsumsi tunggu mbg adita polres economic nol lintas rusak ajak indonesiaemas2045 akal ekosistem mmg anaksehatbangsahebat\n",
      "\n",
      "Topic #2:\n",
      "mbg makansianggratis makan didik prioritas program gratis makanbergizigratis sekolah utama kasih sehat dapet gizi anggar\n",
      "\n",
      "Topic #3:\n",
      "makansianggratis kah syukur kak tahan prabowo 2006 mbg gagas partisipasi hashim juang andal pede gara\n",
      "\n",
      "Topic #4:\n",
      "mbgdorongekonomi umkm pangan makanbergizigratis ekonomi mbg program dukung sehat gizi anak bantu anakanak maju kembang\n"
     ]
    }
   ],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = tf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "      \n",
    "        print(\"\\nTopic #%d:\" % topic_idx )\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda_model, tweet_matrix, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over values of k from 1 to 10\n",
    "K=10\n",
    "inertias_mbg = []\n",
    "\n",
    "for k in range(1, K+1):\n",
    "    # Instantiate the KMeans class with k clusters\n",
    "    ##TODO##\n",
    "    kmeansiris = KMeans(k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    ##TODO##\n",
    "    kmeansiris.fit(df2)\n",
    "\n",
    "    # Store the value of the inertia for this value of k\n",
    "    ##TODO##\n",
    "    inertias_mbg.append(kmeansiris.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "##TODO##\n",
    "plt.figure()\n",
    "plt.plot(range(1, K+1), inertias_mbg, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The elbow method showing the optimal k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeansmbg = KMeans(k,random_state=RANDOMSTATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmbg.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"assigned_cluster\"] = kmeansmbg.predict(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"dataset\\processed\\kmeans_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"dataset\\processed\\kmeans_result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbgsentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
