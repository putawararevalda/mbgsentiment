{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://huggingface.co/LazarusNLP/all-indo-e5-small-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Revalda Putawara\\.conda\\envs\\mbgsentiment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMSTATE=241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first execution : 6 minutes\n",
    "#model = SentenceTransformer('LazarusNLP/all-indo-e5-small-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"dataset\\processed\\processed_data_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "sentences = [df.at[32,\"text\"],df.at[64,\"text\"]]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cs_embeddings'] = df['text_cleaned_stemmed'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle(r\"dataset/processed/text_embeddings.pkl\")\n",
    "#df = pd.read_pickle(r\"dataset/processed/text_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = embeddings[0] - embeddings[1]\n",
    "distance = np.sqrt(np.sum(np.square(temp)))\n",
    "\n",
    "print(\"Euclidean Distance: \", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.iloc[:,-1].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[[\"id\"]]\n",
    "df2 = pd.DataFrame(df.iloc[:,-1].to_list())\n",
    "df_emb = pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://medium.com/analytics-vidhya/topic-modelling-using-word-embeddings-and-latent-dirichlet-allocation-3494778307bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"id.stopwords.02.01.2016.txt\", \"r\")\n",
    "stopword_list = []\n",
    "for line in f:\n",
    "    stripped_line = line.strip()\n",
    "    line_list = stripped_line.split()\n",
    "    stopword_list.append(line_list[0])\n",
    "f.close()\n",
    "\n",
    "stopword_list.append(\"nya\")\n",
    "stopword_list.append(\"ya\")\n",
    "\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword_list = \"ada adalah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_stemmed</th>\n",
       "      <th>mdhugol_sentiment</th>\n",
       "      <th>mdhugol_sentiment_score</th>\n",
       "      <th>indobert_sentiment</th>\n",
       "      <th>indobert_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1880655687866552831</td>\n",
       "      <td>1880655687866552831</td>\n",
       "      <td>2025-01-18T16:37:11.000Z</td>\n",
       "      <td>Ketua Umum PDIP Megawati Soekarnoputri sempat ...</td>\n",
       "      <td>917707622026928129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ketua umum pdip megawati soekarnoputri sempat ...</td>\n",
       "      <td>ketua umum pdip megawati soekarnoputri sempat ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.989232</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1880655628819165446</td>\n",
       "      <td>1880655628819165446</td>\n",
       "      <td>2025-01-18T16:36:57.000Z</td>\n",
       "      <td>@HabilIsl Hold $WOWO dapat airdrop makan siang...</td>\n",
       "      <td>112500656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hold wowo dapat airdrop makansianggratis</td>\n",
       "      <td>hold wowo dapat airdrop makansianggratis</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.912627</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1880655540717842742</td>\n",
       "      <td>1880655540717842742</td>\n",
       "      <td>2025-01-18T16:36:36.000Z</td>\n",
       "      <td>KALAU MBG NYA INI GUE JUGA MAU DUT BIAR GUE MU...</td>\n",
       "      <td>1266052828742602752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kalau mbg nya ini gue juga mau dut biar gue mu...</td>\n",
       "      <td>kalau mbg nya ini gue juga mau dut biar gue mu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.982285</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.999828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1880655480122651014</td>\n",
       "      <td>1880655480122651014</td>\n",
       "      <td>2025-01-18T16:36:22.000Z</td>\n",
       "      <td>@neohistoria_id program MBG tuh B nya Bersyuku...</td>\n",
       "      <td>1838116873331945472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>program mbg tuh b nya bersyukur bukan bergizi</td>\n",
       "      <td>program mbg tuh b nya syukur bukan gizi</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.985092</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.996238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1880655463207117137</td>\n",
       "      <td>1880655463207117137</td>\n",
       "      <td>2025-01-18T16:36:17.000Z</td>\n",
       "      <td>@03__nakula Ga semua anak Indonesia bisa makan...</td>\n",
       "      <td>1511016593353936896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>enggak semua anak indonesia bisa makan siang ...</td>\n",
       "      <td>enggak semua anak indonesia bisa makan siang i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.564391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1                   id  edit_history_tweet_ids  \\\n",
       "0             0             0  1880655687866552831     1880655687866552831   \n",
       "1             1             1  1880655628819165446     1880655628819165446   \n",
       "2             2             2  1880655540717842742     1880655540717842742   \n",
       "3             3             3  1880655480122651014     1880655480122651014   \n",
       "4             4             4  1880655463207117137     1880655463207117137   \n",
       "\n",
       "                 created_at  \\\n",
       "0  2025-01-18T16:37:11.000Z   \n",
       "1  2025-01-18T16:36:57.000Z   \n",
       "2  2025-01-18T16:36:36.000Z   \n",
       "3  2025-01-18T16:36:22.000Z   \n",
       "4  2025-01-18T16:36:17.000Z   \n",
       "\n",
       "                                                text            author_id  \\\n",
       "0  Ketua Umum PDIP Megawati Soekarnoputri sempat ...   917707622026928129   \n",
       "1  @HabilIsl Hold $WOWO dapat airdrop makan siang...            112500656   \n",
       "2  KALAU MBG NYA INI GUE JUGA MAU DUT BIAR GUE MU...  1266052828742602752   \n",
       "3  @neohistoria_id program MBG tuh B nya Bersyuku...  1838116873331945472   \n",
       "4  @03__nakula Ga semua anak Indonesia bisa makan...  1511016593353936896   \n",
       "\n",
       "   Unnamed: 0                                       text_cleaned  \\\n",
       "0         NaN  ketua umum pdip megawati soekarnoputri sempat ...   \n",
       "1         NaN           hold wowo dapat airdrop makansianggratis   \n",
       "2         NaN  kalau mbg nya ini gue juga mau dut biar gue mu...   \n",
       "3         NaN     program mbg tuh b nya bersyukur bukan bergizi    \n",
       "4         NaN   enggak semua anak indonesia bisa makan siang ...   \n",
       "\n",
       "                                text_cleaned_stemmed mdhugol_sentiment  \\\n",
       "0  ketua umum pdip megawati soekarnoputri sempat ...           neutral   \n",
       "1           hold wowo dapat airdrop makansianggratis          positive   \n",
       "2  kalau mbg nya ini gue juga mau dut biar gue mu...          negative   \n",
       "3            program mbg tuh b nya syukur bukan gizi          negative   \n",
       "4  enggak semua anak indonesia bisa makan siang i...          negative   \n",
       "\n",
       "   mdhugol_sentiment_score indobert_sentiment  indobert_sentiment_score  \n",
       "0                 0.989232            Neutral                  0.999553  \n",
       "1                 0.912627            Neutral                  0.999512  \n",
       "2                 0.982285           Negative                  0.999828  \n",
       "3                 0.985092           Negative                  0.996238  \n",
       "4                 0.990175            Neutral                  0.564391  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revalda Putawara\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words=list(my_stop_words), max_features=50000)\n",
    "tweet_matrix = tf_vectorizer.fit_transform(df['text_cleaned_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, learning_method='online', \n",
    "                                          random_state=RANDOMSTATE, verbose=0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03893116, 0.8439003 , 0.03837805, 0.04064049, 0.03815   ],\n",
       "       [0.06897738, 0.72317065, 0.06902739, 0.06906648, 0.0697581 ],\n",
       "       [0.0518288 , 0.05197352, 0.79238007, 0.05208199, 0.05173562],\n",
       "       ...,\n",
       "       [0.7596791 , 0.06005814, 0.06004589, 0.06020113, 0.06001574],\n",
       "       [0.04257712, 0.04196557, 0.04225833, 0.83129004, 0.04190894],\n",
       "       [0.04151527, 0.83384209, 0.0414249 , 0.04205353, 0.0411642 ]],\n",
       "      shape=(375, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = lda.fit(tweet_matrix)\n",
    "lda_matrix = lda_model.transform(tweet_matrix)\n",
    "lda_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01rw', '08surakarta', '0eralatan', ..., 'zhammmmmm', 'zi', 'zone'],\n",
       "      shape=(1904,), dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "mbg makanbergizigratis program indonesia laksana evaluasi visipresidenprabowo makanbegizigratis miskin kasi rakyat uang mending banding anggar\n",
      "\n",
      "Topic #1:\n",
      "makansianggratis mbg gizi anak makan program makanbergizigratis butuh anakanak tanggap anggar udah gua arti ekonomi\n",
      "\n",
      "Topic #2:\n",
      "mbg makansianggratis anak program makan gue enak akal duit dana masuk bilang kantin beli budget\n",
      "\n",
      "Topic #3:\n",
      "program makanbergizigratis mbg makan bikin makansianggratis gratis rumah sekolah orang prabowo banget presiden emang perintah\n",
      "\n",
      "Topic #4:\n",
      "makansianggratis dapet dipake nomer arghhh bingung ril gak mbg kurus enak takut papa gitu king\n"
     ]
    }
   ],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = tf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "      \n",
    "        print(\"\\nTopic #%d:\" % topic_idx )\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda_model, tweet_matrix, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over values of k from 1 to 10\n",
    "K=10\n",
    "inertias_mbg = []\n",
    "\n",
    "for k in range(1, K+1):\n",
    "    # Instantiate the KMeans class with k clusters\n",
    "    ##TODO##\n",
    "    kmeansiris = KMeans(k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    ##TODO##\n",
    "    kmeansiris.fit(df2)\n",
    "\n",
    "    # Store the value of the inertia for this value of k\n",
    "    ##TODO##\n",
    "    inertias_mbg.append(kmeansiris.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "##TODO##\n",
    "plt.figure()\n",
    "plt.plot(range(1, K+1), inertias_mbg, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The elbow method showing the optimal k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeansmbg = KMeans(k,random_state=RANDOMSTATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansmbg.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"assigned_cluster\"] = kmeansmbg.predict(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"dataset\\processed\\kmeans_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"dataset\\processed\\kmeans_result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
